import streamlit as st
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os
import chromadb
import uuid
load_dotenv()
from langchain.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
import re
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage



# Set up LLM
llm = ChatGroq(
    temperature=0,
    groq_api_key=os.getenv("GROQ_API_KEY"),
    model_name="deepseek-r1-distill-llama-70b"
)

# Initialize embeddings model
# embedding_model = OllamaEmbeddings(model="llama3.2:1b")
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")

# Load Chroma vector store
vectorstore = Chroma(persist_directory="chroma-BAAI", embedding_function=embedding_model)


memory = ConversationBufferMemory(
    memory_key="chat_history", return_messages=True
)
# Retrieval-based QA
# retriever = vectorstore.as_retriever()
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5}),
    memory=memory,
)

# Streamlit UI
st.title("AI-Powered Question Answering Chatbot")
st.write("Ask a question based on the uploaded document.")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []


query = st.text_input("Enter your question:")
results = vectorstore.similarity_search(query, k=3)
context = " ".join([doc.page_content for doc in results])
prompt = f'''Based on the following information, answer the question:\n\n{context}\n\nQuestion: {query} 
            if you think query is not related to doc given to ,then please say query is not related to the document given to me
            and give response as per your general knowledge '''

if query:
    with st.spinner("Thinking..."):
        # response = llm.invoke(prompt)
        # cleaned_response = re.sub(r"<think>.*?</think>", "", response.content, flags=re.DOTALL)
        # disclaimer = "\n\n**Disclaimer:** Please verify this information, as it is generated by an AI and may contain errors."
        # full_response = cleaned_response.strip() + disclaimer
        # st.write("### Answer:")
        # st.write(full_response)
        response = qa_chain.invoke({"question": query, "chat_history": st.session_state.chat_history})
        
        # Extract the result
        cleaned_response = re.sub(r"<think>.*?</think>", "", response["answer"], flags=re.DOTALL)

        # Update chat history
        st.session_state.chat_history.append(HumanMessage(query))
        st.session_state.chat_history.append(AIMessage(cleaned_response))
        st.write("### Answer:")
        st.write(cleaned_response)

# Display past conversation
if st.session_state.chat_history:
    st.write("\n\n### Chat History:")
    for message in st.session_state.chat_history:
        role = "ðŸ‘¤ You:" if isinstance(message, HumanMessage) else "ðŸ¤– Bot:"
        st.write(f"{role} {message.content}")
