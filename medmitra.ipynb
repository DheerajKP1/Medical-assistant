{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import chromadb\n",
    "import uuid\n",
    "from langchain.chains import RetrievalQA\n",
    "load_dotenv()\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(\n",
    "    temperature=0,\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name=\"deepseek-r1-distill-llama-70b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\n\\n</think>\\n\\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 6, 'total_tokens': 50, 'completion_time': 0.16, 'prompt_time': 0.003498289, 'queue_time': 0.23289115700000002, 'total_time': 0.163498289}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_492bd52206', 'finish_reason': 'stop', 'logprobs': None}, id='run-84b5518b-a927-4d1a-bf40-0b7cb535cafd-0', usage_metadata={'input_tokens': 6, 'output_tokens': 44, 'total_tokens': 50})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=llm.invoke(\"who are you\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''## Pdf reader\n",
    "\n",
    "loader=PyPDFLoader(\"B09221-eng.pdf\")\n",
    "docs=loader.load()\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "# documents=documents[:5]\n",
    "\n",
    "len(documents)\n",
    "\n",
    "\n",
    "# Initialize embeddings model\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "\n",
    "# Create Chroma vector database\n",
    "vectorstore = Chroma.from_documents(documents, embedding_model, persist_directory=\"chroma_db\")\n",
    "\n",
    "# Save the database\n",
    "vectorstore.persist()\n",
    "\n",
    "# Load the vector store\n",
    "vectorstore = Chroma(persist_directory=\"chroma_db\", embedding_function=embedding_model)\n",
    "\n",
    "query = \"what is the symptom of Ebola\"\n",
    "# results = vectorstore.similarity_search(query, k=3) \n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2}) \n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "\n",
    "\n",
    "response=llm.invoke(\"what is symptom of Ebola\")\n",
    "print(response)\n",
    "\n",
    "context = \" \".join([doc.page_content for doc in results])\n",
    "prompt = f\"Based on the following information, answer the question:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "response = llm.invoke(prompt)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dheeraj\\AppData\\Local\\Temp\\ipykernel_6172\\362889348.py:13: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n",
      "C:\\Users\\dheeraj\\AppData\\Local\\Temp\\ipykernel_6172\\362889348.py:16: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"chroma-BAAI\", embedding_function=embedding_model)\n"
     ]
    }
   ],
   "source": [
    "loader=PyPDFLoader(\"B09221-eng.pdf\")\n",
    "docs=loader.load()\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "\n",
    "# Create Chroma vector database\n",
    "vectorstore = Chroma.from_documents(documents, embedding_model, persist_directory=\"chroma-BAAI\")\n",
    "\n",
    "# Save the database\n",
    "vectorstore.persist()\n",
    "\n",
    "# Load the vector store\n",
    "vectorstore = Chroma(persist_directory=\"chroma-BAAI\", embedding_function=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health Security Agency, United Kingdom of Great Britain and Northern Ireland \n",
      "6. WHO Collaborating Centre for Viral Hemorrhagic Fevers, Centers for Disease Control and Prevention, \n",
      "United States of America\n",
      "Security Agency, United Kingdom of Great Britain and Northern Ireland), Fahn Taweh (National Public Health \n",
      "Institute, Liberia), Babs Verstrepen (Erasmus University Medical Center, Netherlands), Dan Youkee  (Kings \n",
      "College London, United Kingdom of Great Britain and Northern Ireland). \n",
      "WHO Headquarters and Regional Office staff contributors and reviewers:  \n",
      "Mustafa Abdelaziz Aboualy (WHO Regional Office for the Eastern Mediterranean), Rachel Achilla ( WHO \n",
      "Regional Office for Africa), Jeffrey Gilbert (Emerging Diseases and Zoonoses, Geneva, Switzerland), Hieronyma \n",
      "Nelisiwe Gumede -Moeletsi ( WHO Regional Office for Africa ), Jean-Michel Heraud  ( Global Influenza \n",
      "Programme, Geneva, Switzerland), Jean de Dieu Iragena ( WHO Regional Office for Africa),  Kazunobu Kojima \n",
      "(Biosecurity & Health Security Protection, Geneva, Switzerland), Luke Meredith (WHO Regional Office for the \n",
      "Eastern Mediterranean), Walter Oguta (WHO Regional Office for Africa), Boris I. Pavlin ( Acute Events\n",
      "scientists, public health officials, and clinicians with  relevant experience with EBOD and MVD . The draft \n",
      "recommendations were circulated to this group for review and feedback. A first meeting was convened  to \n",
      "discuss key questions and draft recommendations. Following this, the document and recommendations were \n",
      "revised and resubmitted to the expert group for review. A second meeting was then convened to finalise the \n",
      "recommendations and document. During the second meeting, all outstanding questions were discuss ed and \n",
      "consensus across all experts was reached. A final version was circulated to all experts involved for final review.  \n",
      "Document development and review benefited from consultations with WHO technical teams  and experts in \n",
      "WHO headquarters, the WHO Regional Office for Africa and the WHO Office for the Eastern Mediterranean \n",
      "who have appropriate experience in the areas of laboratory testing, surveillance, clinical management,\n"
     ]
    }
   ],
   "source": [
    "query = \"what is WHO\"\n",
    "results = vectorstore.similarity_search(query, k=3) \n",
    "# print(results)\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \" \".join([doc.page_content for doc in results])\n",
    "prompt = f\"Based on the following information, answer the question:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}),\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Bot: \n",
      "\n",
      "WHO stands for the World Health Organization. It is a specialized agency of the United Nations responsible for international public health. It is concerned with global health matters, particularly infectious diseases like Ebola and Marburg, and works to develop and implement policies that promote health, keep the world safe, and serve the vulnerable.\n",
      "\n",
      "🤖 Bot: \n",
      "\n",
      "How are you?\n",
      "\n",
      "🤖 Bot: \n",
      "\n",
      "LLM in this context likely stands for a specific program or initiative within the World Health Organization, possibly related to laboratory strengthening or logistical management, which are crucial for addressing public health emergencies like Ebola and Marburg.\n",
      "\n",
      "🤖 Bot: <think>\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "while True:\n",
    "    query = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "    \n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    # Invoke the QA chain\n",
    "    response = qa_chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "    \n",
    "    # Extract and clean response\n",
    "    cleaned_response = re.sub(r\"<think>.*?</think>\", \"\", response[\"answer\"], flags=re.DOTALL)\n",
    "    \n",
    "    # Update chat history\n",
    "    chat_history.append(HumanMessage(query))\n",
    "    chat_history.append(AIMessage(cleaned_response))\n",
    "\n",
    "    # Display response\n",
    "    print(\"\\n🤖 Bot:\", cleaned_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
